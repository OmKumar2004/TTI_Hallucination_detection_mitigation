{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f0551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from torchvision import transforms\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "# import random\n",
    "\n",
    "# MY_TOKEN = \"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\"\n",
    "# # HuggingFace dataset\n",
    "# dataset = load_dataset(\"nlphuji/flickr30k\", split=\"train[:1%], use_auth_token=MY_TOKEN\")  # use a subset to test first\n",
    "\n",
    "# # Preprocessing\n",
    "# image_size = 512\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((image_size, image_size)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1] for SD\n",
    "# ])\n",
    "\n",
    "# # Use CLIP tokenizer for prompt text\n",
    "# from transformers import CLIPTokenizer\n",
    "# tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# class FlickrDataset(Dataset):\n",
    "#     def __init__(self, dataset, transform, tokenizer):\n",
    "#         self.dataset = dataset\n",
    "#         self.transform = transform\n",
    "#         self.tokenizer = tokenizer\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         entry = self.dataset[idx]\n",
    "#         image = self.transform(entry['image'].convert(\"RGB\"))\n",
    "#         # caption = entry['caption'][0]\n",
    "#         caption = random.choice(entry['caption'])\n",
    "#         tokens = self.tokenizer(caption, padding=\"max_length\", truncation=True, max_length=77, return_tensors=\"pt\")\n",
    "#         return {\n",
    "#             \"image\": image,\n",
    "#             \"input_ids\": tokens[\"input_ids\"].squeeze(0),\n",
    "#             \"attention_mask\": tokens[\"attention_mask\"].squeeze(0),\n",
    "#             \"text\": caption\n",
    "#         }\n",
    "\n",
    "# # Create DataLoader\n",
    "# flickr_dataset = FlickrDataset(dataset, transform, tokenizer)\n",
    "# dataloader = DataLoader(flickr_dataset, batch_size=2, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d2220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "SAVE_PATH = \"flickr30k_preprocessed_1pct_comp.pt\"\n",
    "loaded_data = torch.load(SAVE_PATH)\n",
    "\n",
    "class CachedFlickrDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Usage\n",
    "dataset = CachedFlickrDataset(loaded_data)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch['text'])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb76313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/host_root/data-a40/kartik/om/DL_Project/Finetuning/dl_proj/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_train_steps):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mdataloader\u001b[49m):\n\u001b[1;32m     74\u001b[0m         images \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[1;32m     75\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import necessary diffusion components from diffusers and transformers\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL, DDPMScheduler\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "# Import our custom attention utilities.\n",
    "from new_attention_utils import (\n",
    "    register_attention_hooks,\n",
    "    compute_hallucination_penalty,\n",
    "    get_last_cross_attention_resized,\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration & Model Loading\n",
    "# -----------------------------\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_path = \"CompVis/stable-diffusion-v1-4\"\n",
    "batch_size = 32  # For demonstration; adjust as needed\n",
    "\n",
    "# Load components (with torch_dtype=torch.float16)\n",
    "unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\", torch_dtype=torch.float16).to(device)\n",
    "vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\", torch_dtype=torch.float16).to(device)\n",
    "scheduler = DDPMScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\n",
    "\n",
    "# For text conditioning, use CLIP's tokenizer and text encoder.\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_path, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_path, subfolder=\"text_encoder\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# Inject LoRA into UNet\n",
    "# -----------------------------\n",
    "from peft import get_peft_model, LoraConfig\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "unet = get_peft_model(unet, lora_config)\n",
    "unet.train()\n",
    "\n",
    "# Register attention hooks so that our custom utility collects attention maps.\n",
    "register_attention_hooks(unet)\n",
    "\n",
    "# -----------------------------\n",
    "# Create Dataset and DataLoader\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helper for Prompt Encoding\n",
    "# -----------------------------\n",
    "# Define a projector that maps the attribute vector to the text encoder's hidden dimension.\n",
    "hidden_dim = 768\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training Loop\n",
    "# -----------------------------\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=1e-6)\n",
    "num_train_steps = 10  # For testing; adjust as needed\n",
    "\n",
    "for epoch in range(num_train_steps):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    for batch in tqdm(dataloader):\n",
    "        images = batch[\"image\"].to(device, dtype=torch.float16)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Encode images into latents via VAE (no grad through VAE)\n",
    "        with torch.no_grad():\n",
    "            latents = vae.encode(images).latent_dist.sample() * vae.config.scaling_factor\n",
    "        \n",
    "        noise = torch.randn_like(latents)\n",
    "        bsz = latents.shape[0]\n",
    "        timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (bsz,), device=device).long()\n",
    "        \n",
    "        # Add noise (forward diffusion)\n",
    "        noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "        noisy_latents.requires_grad_(True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoder_hidden_states = text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        \n",
    "        # Forward pass through UNet (this call triggers attention hooks)\n",
    "        model_output = unet(noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states).sample\n",
    "        \n",
    "        # Standard denoising (MSE) loss between predicted noise and true noise.\n",
    "        loss_mse = F.mse_loss(model_output.float(), noise.float(), reduction='mean')\n",
    "        \n",
    "        # Extract and resize attention maps (to a chosen target shape; e.g., 32x32).\n",
    "        try:\n",
    "            attn_maps = get_last_cross_attention_resized(target_shape=(32, 32))\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            attn_maps = torch.zeros(1, 1, noisy_latents.shape[-2], noisy_latents.shape[-1]).to(device)\n",
    "        \n",
    "        # Compute hallucination loss:\n",
    "        # According to our formulation, compute A_sum from attention maps, then M_halluc = 1 - clip(A_sum,0,1)\n",
    "        # and then penalize the predicted features model_output with M_halluc.\n",
    "        loss_hallu = compute_hallucination_penalty(attn_maps, model_output)\n",
    "        \n",
    "        # Total loss = denoising loss + lambda * hallucination loss\n",
    "        lambda_h = 0.3\n",
    "        loss = loss_mse + lambda_h * loss_hallu\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        tqdm.write(f\"Loss: {loss.item():.6f} | MSE: {loss_mse.item():.6f} | Hallu: {loss_hallu.item():.6f}\")\n",
    "    \n",
    "    # Optionally, reset attention storage here if necessary.\n",
    "    \n",
    "# Save the fine-tuned LoRA weights.\n",
    "save_path = \"./lora_finetuned_f3_all\"\n",
    "unet.save_pretrained(save_path)\n",
    "print(f\"LoRA weights saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8006cc87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
